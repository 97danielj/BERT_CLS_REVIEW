{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2c3fc4",
   "metadata": {},
   "source": [
    "# 1) 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b2c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #토크나이저 => 단어집합을 생성시켜준다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 벡터를 패딩시켜준다.\n",
    "from tensorflow.keras.utils import to_categorical #원-핫 인코딩|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c66d95",
   "metadata": {},
   "source": [
    "# 코퍼스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9400d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
    "그의 말이 법이다\\n\n",
    "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b94af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n",
      "단어집합의 크기 12\n"
     ]
    }
   ],
   "source": [
    "#단어집합을 생성하고 크기를 확인\n",
    "# 1. 정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text]) #단어 집합을 생성\n",
    "print(tokenizer.word_index)\n",
    "vocab_size = len(tokenizer.word_index)+1 #패딩을 위한 0을 고려\n",
    "print('단어집합의 크기 %d' % vocab_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc0901",
   "metadata": {},
   "source": [
    "# 훈련데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d5d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 사용할 샘플의 개수: 11\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): # 줄바꿈 문자 기준으로 코퍼스 분리\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0] #문장을 정수 토큰화\n",
    "    for i in range(1, len(encoded)): # 1~문장의 길이\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "print('학습에 사용할 샘플의 개수: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ac27f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3],\n",
       " [2, 3, 1],\n",
       " [2, 3, 1, 4],\n",
       " [2, 3, 1, 4, 5],\n",
       " [6, 1],\n",
       " [6, 1, 7],\n",
       " [8, 1],\n",
       " [8, 1, 9],\n",
       " [8, 1, 9, 10],\n",
       " [8, 1, 9, 10, 1],\n",
       " [8, 1, 9, 10, 1, 11]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences\n",
    "#위 데이터는 레이블로 사용될 단어를 분리하지 않은 훈련데이터이다.\n",
    "#전체 훈련데이터에 대해서 맨 우측에 있는 단어만 대해서만 레이블로 븐리해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ed72a",
   "metadata": {},
   "source": [
    "# 훈련데이터 길이 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d539ff56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 6\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f5b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 샘플의 길이를 6으로 맞춰준다.\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd59585c",
   "metadata": {},
   "source": [
    "pad_sequences()는 모든 샘플에 대해서 0을 사용하여 길이를 맞춰줍니다. maxlen의 값으로 6을 주면 모든 샘플의 길이를 6으로 맞춰주며, padding의 인자로 'pre'를 주면 길이가 6보다 짧은 샘플의 앞에 0으로 채웁니다. 전체 훈련 데이터를 출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a1b97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  2,  3,  1],\n",
       "       [ 0,  0,  2,  3,  1,  4],\n",
       "       [ 0,  2,  3,  1,  4,  5],\n",
       "       [ 0,  0,  0,  0,  6,  1],\n",
       "       [ 0,  0,  0,  6,  1,  7],\n",
       "       [ 0,  0,  0,  0,  8,  1],\n",
       "       [ 0,  0,  0,  8,  1,  9],\n",
       "       [ 0,  0,  8,  1,  9, 10],\n",
       "       [ 0,  8,  1,  9, 10,  1],\n",
       "       [ 8,  1,  9, 10,  1, 11]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46968b",
   "metadata": {},
   "source": [
    "# 각 샘플의 마지막 단어를 레이블로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c7527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  2,  3,  1],\n",
       "       [ 0,  0,  2,  3,  1,  4],\n",
       "       [ 0,  2,  3,  1,  4,  5],\n",
       "       [ 0,  0,  0,  0,  6,  1],\n",
       "       [ 0,  0,  0,  6,  1,  7],\n",
       "       [ 0,  0,  0,  0,  8,  1],\n",
       "       [ 0,  0,  0,  8,  1,  9],\n",
       "       [ 0,  0,  8,  1,  9, 10],\n",
       "       [ 0,  8,  1,  9, 10,  1],\n",
       "       [ 8,  1,  9, 10,  1, 11]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#레이블븐리는 Numpy를 사용해서 가능\n",
    "import numpy as np\n",
    "np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84b382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : 리스트의 마지막 값을 제외하고 저장 -> 입력데이터\n",
    "# y : 리스트의 마지막 값만을 저장 -> 레이블 \n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1] # 2차원 배열\n",
    "y = sequences[:,-1] # 1차원 벡터(레이블)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370892e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  2,  3,  1],\n",
       "       [ 0,  2,  3,  1,  4],\n",
       "       [ 0,  0,  0,  0,  6],\n",
       "       [ 0,  0,  0,  6,  1],\n",
       "       [ 0,  0,  0,  0,  8],\n",
       "       [ 0,  0,  0,  8,  1],\n",
       "       [ 0,  0,  8,  1,  9],\n",
       "       [ 0,  8,  1,  9, 10],\n",
       "       [ 8,  1,  9, 10,  1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f704ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1,  4,  5,  1,  7,  1,  9, 10,  1, 11])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef3e37",
   "metadata": {},
   "source": [
    "# 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9639f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블이 분리되었습니다. RNN 모델에 훈련 데이터를 훈련 시키기 전에 레이블에 대해서 원-핫 인코딩을 수행합니다.\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87eea290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9df6c17",
   "metadata": {},
   "source": [
    "갑분 정리\n",
    "RNNLM은 입력 길이에 상관없이 시점 개념을 적용하여 다음 단어를 예측 가능하다.\n",
    "RNNLM은 기본적을 예측과정에서 이전 출력을 현재시점의 입력으로 합니다.\n",
    "교사 강요 방식 : 테스트 과정에서 t 시점의 출력이 t+1 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 때 사용하는 훈련 기법입니다. 단 훈련시에는 이전 시점의 출력이 아닌 이전 시점의 레이블(알고 있는 정답)을 입력을 사용한다.\n",
    "cf) 데이터 처리 와 모델링\n",
    "1. 데이터\n",
    "    1. 훈련 코퍼스 정수 인코딩 -> Tokenizer\n",
    "    2. 인코딩 벡터를 동일 길이로 패딩 -> pad_sequences\n",
    "    3. 인코딩 벡터를 입력 데이터 X와 출력 레이블y 로 분리\n",
    "    4. 출력 레이블 y를 원-핫인코딩\n",
    "2. 모델링\n",
    "    input layer : 입력 벡터(V)(=단어 집합의 크기)\n",
    "    Embedding layer : 임베딩 층 (V, M), 임베딩 벡터 : M\n",
    "    Hidden layer : 은닉 상태 ht \n",
    "    Output layer : 출력층(소프트 맥스) / 입력 벡터의크기(V) / cross-entropy 지수로 나온다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f086534",
   "metadata": {},
   "source": [
    "# 모델설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "733dea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6339f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 - 1s - loss: 2.4713 - accuracy: 0.0909 - 947ms/epoch - 947ms/step\n",
      "Epoch 2/200\n",
      "1/1 - 0s - loss: 2.4575 - accuracy: 0.3636 - 2ms/epoch - 2ms/step\n",
      "Epoch 3/200\n",
      "1/1 - 0s - loss: 2.4439 - accuracy: 0.3636 - 2ms/epoch - 2ms/step\n",
      "Epoch 4/200\n",
      "1/1 - 0s - loss: 2.4304 - accuracy: 0.3636 - 2ms/epoch - 2ms/step\n",
      "Epoch 5/200\n",
      "1/1 - 0s - loss: 2.4169 - accuracy: 0.6364 - 2ms/epoch - 2ms/step\n",
      "Epoch 6/200\n",
      "1/1 - 0s - loss: 2.4032 - accuracy: 0.6364 - 2ms/epoch - 2ms/step\n",
      "Epoch 7/200\n",
      "1/1 - 0s - loss: 2.3892 - accuracy: 0.6364 - 2ms/epoch - 2ms/step\n",
      "Epoch 8/200\n",
      "1/1 - 0s - loss: 2.3748 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 9/200\n",
      "1/1 - 0s - loss: 2.3600 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 10/200\n",
      "1/1 - 0s - loss: 2.3447 - accuracy: 0.6364 - 998us/epoch - 998us/step\n",
      "Epoch 11/200\n",
      "1/1 - 0s - loss: 2.3288 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 12/200\n",
      "1/1 - 0s - loss: 2.3123 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 13/200\n",
      "1/1 - 0s - loss: 2.2951 - accuracy: 0.4545 - 998us/epoch - 998us/step\n",
      "Epoch 14/200\n",
      "1/1 - 0s - loss: 2.2771 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 15/200\n",
      "1/1 - 0s - loss: 2.2584 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 16/200\n",
      "1/1 - 0s - loss: 2.2389 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 17/200\n",
      "1/1 - 0s - loss: 2.2186 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 18/200\n",
      "1/1 - 0s - loss: 2.1975 - accuracy: 0.4545 - 998us/epoch - 998us/step\n",
      "Epoch 19/200\n",
      "1/1 - 0s - loss: 2.1756 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 20/200\n",
      "1/1 - 0s - loss: 2.1529 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 21/200\n",
      "1/1 - 0s - loss: 2.1295 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 22/200\n",
      "1/1 - 0s - loss: 2.1055 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 23/200\n",
      "1/1 - 0s - loss: 2.0810 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 24/200\n",
      "1/1 - 0s - loss: 2.0560 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 25/200\n",
      "1/1 - 0s - loss: 2.0308 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 26/200\n",
      "1/1 - 0s - loss: 2.0054 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 27/200\n",
      "1/1 - 0s - loss: 1.9802 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 28/200\n",
      "1/1 - 0s - loss: 1.9552 - accuracy: 0.4545 - 998us/epoch - 998us/step\n",
      "Epoch 29/200\n",
      "1/1 - 0s - loss: 1.9307 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 30/200\n",
      "1/1 - 0s - loss: 1.9067 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 31/200\n",
      "1/1 - 0s - loss: 1.8835 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 32/200\n",
      "1/1 - 0s - loss: 1.8611 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 33/200\n",
      "1/1 - 0s - loss: 1.8395 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 34/200\n",
      "1/1 - 0s - loss: 1.8186 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 35/200\n",
      "1/1 - 0s - loss: 1.7981 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 36/200\n",
      "1/1 - 0s - loss: 1.7779 - accuracy: 0.4545 - 999us/epoch - 999us/step\n",
      "Epoch 37/200\n",
      "1/1 - 0s - loss: 1.7578 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 38/200\n",
      "1/1 - 0s - loss: 1.7374 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 39/200\n",
      "1/1 - 0s - loss: 1.7167 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 40/200\n",
      "1/1 - 0s - loss: 1.6954 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 41/200\n",
      "1/1 - 0s - loss: 1.6735 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 42/200\n",
      "1/1 - 0s - loss: 1.6510 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 43/200\n",
      "1/1 - 0s - loss: 1.6278 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 44/200\n",
      "1/1 - 0s - loss: 1.6042 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 45/200\n",
      "1/1 - 0s - loss: 1.5803 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 46/200\n",
      "1/1 - 0s - loss: 1.5560 - accuracy: 0.4545 - 997us/epoch - 997us/step\n",
      "Epoch 47/200\n",
      "1/1 - 0s - loss: 1.5315 - accuracy: 0.4545 - 998us/epoch - 998us/step\n",
      "Epoch 48/200\n",
      "1/1 - 0s - loss: 1.5069 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 49/200\n",
      "1/1 - 0s - loss: 1.4822 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 50/200\n",
      "1/1 - 0s - loss: 1.4575 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 51/200\n",
      "1/1 - 0s - loss: 1.4327 - accuracy: 0.5455 - 998us/epoch - 998us/step\n",
      "Epoch 52/200\n",
      "1/1 - 0s - loss: 1.4079 - accuracy: 0.5455 - 997us/epoch - 997us/step\n",
      "Epoch 53/200\n",
      "1/1 - 0s - loss: 1.3831 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 54/200\n",
      "1/1 - 0s - loss: 1.3582 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 55/200\n",
      "1/1 - 0s - loss: 1.3332 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 56/200\n",
      "1/1 - 0s - loss: 1.3083 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 57/200\n",
      "1/1 - 0s - loss: 1.2833 - accuracy: 0.5455 - 997us/epoch - 997us/step\n",
      "Epoch 58/200\n",
      "1/1 - 0s - loss: 1.2584 - accuracy: 0.5455 - 997us/epoch - 997us/step\n",
      "Epoch 59/200\n",
      "1/1 - 0s - loss: 1.2336 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 60/200\n",
      "1/1 - 0s - loss: 1.2089 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 61/200\n",
      "1/1 - 0s - loss: 1.1845 - accuracy: 0.5455 - 998us/epoch - 998us/step\n",
      "Epoch 62/200\n",
      "1/1 - 0s - loss: 1.1604 - accuracy: 0.5455 - 2ms/epoch - 2ms/step\n",
      "Epoch 63/200\n",
      "1/1 - 0s - loss: 1.1367 - accuracy: 0.6364 - 998us/epoch - 998us/step\n",
      "Epoch 64/200\n",
      "1/1 - 0s - loss: 1.1134 - accuracy: 0.6364 - 2ms/epoch - 2ms/step\n",
      "Epoch 65/200\n",
      "1/1 - 0s - loss: 1.0906 - accuracy: 0.6364 - 997us/epoch - 997us/step\n",
      "Epoch 66/200\n",
      "1/1 - 0s - loss: 1.0683 - accuracy: 0.6364 - 998us/epoch - 998us/step\n",
      "Epoch 67/200\n",
      "1/1 - 0s - loss: 1.0465 - accuracy: 0.6364 - 2ms/epoch - 2ms/step\n",
      "Epoch 68/200\n",
      "1/1 - 0s - loss: 1.0252 - accuracy: 0.6364 - 998us/epoch - 998us/step\n",
      "Epoch 69/200\n",
      "1/1 - 0s - loss: 1.0044 - accuracy: 0.7273 - 998us/epoch - 998us/step\n",
      "Epoch 70/200\n",
      "1/1 - 0s - loss: 0.9841 - accuracy: 0.7273 - 997us/epoch - 997us/step\n",
      "Epoch 71/200\n",
      "1/1 - 0s - loss: 0.9643 - accuracy: 0.7273 - 2ms/epoch - 2ms/step\n",
      "Epoch 72/200\n",
      "1/1 - 0s - loss: 0.9450 - accuracy: 0.7273 - 2ms/epoch - 2ms/step\n",
      "Epoch 73/200\n",
      "1/1 - 0s - loss: 0.9262 - accuracy: 0.7273 - 997us/epoch - 997us/step\n",
      "Epoch 74/200\n",
      "1/1 - 0s - loss: 0.9078 - accuracy: 0.7273 - 998us/epoch - 998us/step\n",
      "Epoch 75/200\n",
      "1/1 - 0s - loss: 0.8898 - accuracy: 0.7273 - 2ms/epoch - 2ms/step\n",
      "Epoch 76/200\n",
      "1/1 - 0s - loss: 0.8722 - accuracy: 0.7273 - 997us/epoch - 997us/step\n",
      "Epoch 77/200\n",
      "1/1 - 0s - loss: 0.8551 - accuracy: 0.7273 - 2ms/epoch - 2ms/step\n",
      "Epoch 78/200\n",
      "1/1 - 0s - loss: 0.8383 - accuracy: 0.7273 - 2ms/epoch - 2ms/step\n",
      "Epoch 79/200\n",
      "1/1 - 0s - loss: 0.8220 - accuracy: 0.7273 - 997us/epoch - 997us/step\n",
      "Epoch 80/200\n",
      "1/1 - 0s - loss: 0.8059 - accuracy: 0.8182 - 998us/epoch - 998us/step\n",
      "Epoch 81/200\n",
      "1/1 - 0s - loss: 0.7903 - accuracy: 0.8182 - 998us/epoch - 998us/step\n",
      "Epoch 82/200\n",
      "1/1 - 0s - loss: 0.7750 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 83/200\n",
      "1/1 - 0s - loss: 0.7600 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 84/200\n",
      "1/1 - 0s - loss: 0.7453 - accuracy: 0.8182 - 997us/epoch - 997us/step\n",
      "Epoch 85/200\n",
      "1/1 - 0s - loss: 0.7310 - accuracy: 0.8182 - 997us/epoch - 997us/step\n",
      "Epoch 86/200\n",
      "1/1 - 0s - loss: 0.7169 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 87/200\n",
      "1/1 - 0s - loss: 0.7031 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 88/200\n",
      "1/1 - 0s - loss: 0.6897 - accuracy: 0.8182 - 998us/epoch - 998us/step\n",
      "Epoch 89/200\n",
      "1/1 - 0s - loss: 0.6764 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 90/200\n",
      "1/1 - 0s - loss: 0.6635 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 91/200\n",
      "1/1 - 0s - loss: 0.6508 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 92/200\n",
      "1/1 - 0s - loss: 0.6384 - accuracy: 0.8182 - 998us/epoch - 998us/step\n",
      "Epoch 93/200\n",
      "1/1 - 0s - loss: 0.6263 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 94/200\n",
      "1/1 - 0s - loss: 0.6144 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 95/200\n",
      "1/1 - 0s - loss: 0.6027 - accuracy: 0.8182 - 997us/epoch - 997us/step\n",
      "Epoch 96/200\n",
      "1/1 - 0s - loss: 0.5913 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 97/200\n",
      "1/1 - 0s - loss: 0.5801 - accuracy: 0.8182 - 997us/epoch - 997us/step\n",
      "Epoch 98/200\n",
      "1/1 - 0s - loss: 0.5691 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 99/200\n",
      "1/1 - 0s - loss: 0.5584 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 100/200\n",
      "1/1 - 0s - loss: 0.5478 - accuracy: 0.8182 - 998us/epoch - 998us/step\n",
      "Epoch 101/200\n",
      "1/1 - 0s - loss: 0.5375 - accuracy: 0.8182 - 997us/epoch - 997us/step\n",
      "Epoch 102/200\n",
      "1/1 - 0s - loss: 0.5274 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200\n",
      "1/1 - 0s - loss: 0.5175 - accuracy: 0.8182 - 2ms/epoch - 2ms/step\n",
      "Epoch 104/200\n",
      "1/1 - 0s - loss: 0.5078 - accuracy: 0.9091 - 2ms/epoch - 2ms/step\n",
      "Epoch 105/200\n",
      "1/1 - 0s - loss: 0.4982 - accuracy: 0.9091 - 2ms/epoch - 2ms/step\n",
      "Epoch 106/200\n",
      "1/1 - 0s - loss: 0.4889 - accuracy: 0.9091 - 2ms/epoch - 2ms/step\n",
      "Epoch 107/200\n",
      "1/1 - 0s - loss: 0.4797 - accuracy: 0.9091 - 2ms/epoch - 2ms/step\n",
      "Epoch 108/200\n",
      "1/1 - 0s - loss: 0.4707 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 109/200\n",
      "1/1 - 0s - loss: 0.4619 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 110/200\n",
      "1/1 - 0s - loss: 0.4533 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 111/200\n",
      "1/1 - 0s - loss: 0.4448 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 112/200\n",
      "1/1 - 0s - loss: 0.4365 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 113/200\n",
      "1/1 - 0s - loss: 0.4283 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 114/200\n",
      "1/1 - 0s - loss: 0.4203 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 115/200\n",
      "1/1 - 0s - loss: 0.4124 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 116/200\n",
      "1/1 - 0s - loss: 0.4046 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 117/200\n",
      "1/1 - 0s - loss: 0.3970 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 118/200\n",
      "1/1 - 0s - loss: 0.3896 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 119/200\n",
      "1/1 - 0s - loss: 0.3822 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 120/200\n",
      "1/1 - 0s - loss: 0.3750 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 121/200\n",
      "1/1 - 0s - loss: 0.3679 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 122/200\n",
      "1/1 - 0s - loss: 0.3610 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 123/200\n",
      "1/1 - 0s - loss: 0.3541 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 124/200\n",
      "1/1 - 0s - loss: 0.3473 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 125/200\n",
      "1/1 - 0s - loss: 0.3407 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 126/200\n",
      "1/1 - 0s - loss: 0.3342 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 127/200\n",
      "1/1 - 0s - loss: 0.3277 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 128/200\n",
      "1/1 - 0s - loss: 0.3214 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 129/200\n",
      "1/1 - 0s - loss: 0.3152 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 130/200\n",
      "1/1 - 0s - loss: 0.3091 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 131/200\n",
      "1/1 - 0s - loss: 0.3030 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 132/200\n",
      "1/1 - 0s - loss: 0.2971 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 133/200\n",
      "1/1 - 0s - loss: 0.2912 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 134/200\n",
      "1/1 - 0s - loss: 0.2855 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 135/200\n",
      "1/1 - 0s - loss: 0.2798 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 136/200\n",
      "1/1 - 0s - loss: 0.2742 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 137/200\n",
      "1/1 - 0s - loss: 0.2687 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 138/200\n",
      "1/1 - 0s - loss: 0.2633 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 139/200\n",
      "1/1 - 0s - loss: 0.2580 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 140/200\n",
      "1/1 - 0s - loss: 0.2527 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 141/200\n",
      "1/1 - 0s - loss: 0.2475 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 142/200\n",
      "1/1 - 0s - loss: 0.2425 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 143/200\n",
      "1/1 - 0s - loss: 0.2375 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 144/200\n",
      "1/1 - 0s - loss: 0.2326 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 145/200\n",
      "1/1 - 0s - loss: 0.2277 - accuracy: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 146/200\n",
      "1/1 - 0s - loss: 0.2230 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 147/200\n",
      "1/1 - 0s - loss: 0.2183 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 148/200\n",
      "1/1 - 0s - loss: 0.2137 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 149/200\n",
      "1/1 - 0s - loss: 0.2092 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 150/200\n",
      "1/1 - 0s - loss: 0.2048 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 151/200\n",
      "1/1 - 0s - loss: 0.2004 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 152/200\n",
      "1/1 - 0s - loss: 0.1962 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 153/200\n",
      "1/1 - 0s - loss: 0.1920 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 154/200\n",
      "1/1 - 0s - loss: 0.1879 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 155/200\n",
      "1/1 - 0s - loss: 0.1838 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 156/200\n",
      "1/1 - 0s - loss: 0.1799 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 157/200\n",
      "1/1 - 0s - loss: 0.1760 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 158/200\n",
      "1/1 - 0s - loss: 0.1722 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 159/200\n",
      "1/1 - 0s - loss: 0.1685 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 160/200\n",
      "1/1 - 0s - loss: 0.1648 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 161/200\n",
      "1/1 - 0s - loss: 0.1613 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 162/200\n",
      "1/1 - 0s - loss: 0.1578 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 163/200\n",
      "1/1 - 0s - loss: 0.1544 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 164/200\n",
      "1/1 - 0s - loss: 0.1511 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 165/200\n",
      "1/1 - 0s - loss: 0.1478 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 166/200\n",
      "1/1 - 0s - loss: 0.1446 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 167/200\n",
      "1/1 - 0s - loss: 0.1415 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 168/200\n",
      "1/1 - 0s - loss: 0.1385 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 169/200\n",
      "1/1 - 0s - loss: 0.1356 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 170/200\n",
      "1/1 - 0s - loss: 0.1327 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 171/200\n",
      "1/1 - 0s - loss: 0.1299 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 172/200\n",
      "1/1 - 0s - loss: 0.1271 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 173/200\n",
      "1/1 - 0s - loss: 0.1244 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 174/200\n",
      "1/1 - 0s - loss: 0.1218 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 175/200\n",
      "1/1 - 0s - loss: 0.1193 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 176/200\n",
      "1/1 - 0s - loss: 0.1168 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 177/200\n",
      "1/1 - 0s - loss: 0.1144 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 178/200\n",
      "1/1 - 0s - loss: 0.1121 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 179/200\n",
      "1/1 - 0s - loss: 0.1098 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 180/200\n",
      "1/1 - 0s - loss: 0.1076 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 181/200\n",
      "1/1 - 0s - loss: 0.1054 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 182/200\n",
      "1/1 - 0s - loss: 0.1033 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 183/200\n",
      "1/1 - 0s - loss: 0.1013 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 184/200\n",
      "1/1 - 0s - loss: 0.0993 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 185/200\n",
      "1/1 - 0s - loss: 0.0974 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 186/200\n",
      "1/1 - 0s - loss: 0.0955 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 187/200\n",
      "1/1 - 0s - loss: 0.0936 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 188/200\n",
      "1/1 - 0s - loss: 0.0919 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 189/200\n",
      "1/1 - 0s - loss: 0.0901 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 190/200\n",
      "1/1 - 0s - loss: 0.0885 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 191/200\n",
      "1/1 - 0s - loss: 0.0868 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 192/200\n",
      "1/1 - 0s - loss: 0.0852 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 193/200\n",
      "1/1 - 0s - loss: 0.0837 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 194/200\n",
      "1/1 - 0s - loss: 0.0822 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 195/200\n",
      "1/1 - 0s - loss: 0.0807 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 196/200\n",
      "1/1 - 0s - loss: 0.0793 - accuracy: 1.0000 - 997us/epoch - 997us/step\n",
      "Epoch 197/200\n",
      "1/1 - 0s - loss: 0.0779 - accuracy: 1.0000 - 998us/epoch - 998us/step\n",
      "Epoch 198/200\n",
      "1/1 - 0s - loss: 0.0765 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 199/200\n",
      "1/1 - 0s - loss: 0.0752 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 200/200\n",
      "1/1 - 0s - loss: 0.0739 - accuracy: 1.0000 - 997us/epoch - 997us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x228b9498e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RNN모델에 데이터를 훈련시킵니다.\n",
    "#다대일 구조의 RNN을 사용하빈다.\n",
    "embedding_dim = 10 # 임베딩 벡터의 차원\n",
    "hidden_units = 32 #은닉상태의 크기\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim)) #임베딩층 구성\n",
    "model.add(SimpleRNN(hidden_units)) #은닉상태 크기\n",
    "model.add(Dense(vocab_size, activation='softmax')) #출력층 (FC/ 출력 벡터로 단어크기만큼)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X, y, epochs=200, verbose=2)\n",
    "\n",
    "#마지막 시점에서 모든 가능한 단어 중 하나의 단어를 예측하는 다중 클래스 분류 문제를 수행하는 모델"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "867c78c6",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7308c64",
   "metadata": {},
   "source": [
    "# 문장을 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63020e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word #시작 단어\n",
    "    sentence = ''\n",
    "    # n번 반복 4번 반복 현재단어(t) +예측단어(t+1)+예측단어(t+2)+예측단어(t+3)+예측단어(t+4)\n",
    "    for _ in range(n):\n",
    "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0] #현재 단어 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') #최대길이 5로 padding / 왜 5인가. 훈련 샘플 최대 길이 6에서 맨마지막 레이블 을 빼버렸으니 5이다.\n",
    "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0) # 엔트로파 벡터(에측 벡터)\n",
    "        result= np.argmax(result, axis=1) #예측 벡터의 가장큰 값의 인덱스를 저장\n",
    "        \n",
    "    \n",
    "        for word, index in tokenizer.word_index.items(): #단어사전에서\n",
    "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
    "            if index == result: #예측한 단어와 동일한 단어가 있다면\n",
    "                break\n",
    "\n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        current_word = current_word + ' '  + word\n",
    "\n",
    "        # 예측 단어를 문장에 저장\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da3cff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경마장에 있는 말이 뛰고 있다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '경마장에', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80edca7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경마장에 있는 말이\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '경마장에', 2)) # 두번 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d7c8fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'말이': 1,\n",
       " '경마장에': 2,\n",
       " '있는': 3,\n",
       " '뛰고': 4,\n",
       " '있다': 5,\n",
       " '그의': 6,\n",
       " '법이다': 7,\n",
       " '가는': 8,\n",
       " '고와야': 9,\n",
       " '오는': 10,\n",
       " '곱다': 11}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c22be2e",
   "metadata": {},
   "source": [
    "샘플이 어떠한가\n",
    "초기단어 + 여려개의 단어\n",
    "에서\n",
    "X = [:-1] 과 y = [-1]로 분리시켰다.\n",
    "y는 초기단어 제외 원-핫벡터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c0988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
